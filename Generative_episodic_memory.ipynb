{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a331c284",
   "metadata": {},
   "source": [
    "Base model of the architechture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f4da6",
   "metadata": {},
   "source": [
    "The base model consists of a VQ-VAE and PixelCNN introduced in the Neural Discrete Representation Learning (van den Oord et al, NeurIPS 2017).\n",
    "\n",
    "The codes in this notebook are mainly adopted from the notebook created by Am√©lie Royer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39752ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.saving.save import load_model\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "import os\n",
    "#os.chdir('C:/Users/ayata/Desktop/Vorlesungen/TNS/VQVAE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17de66",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679473e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_LATENT_K = 20                 # Number of codebook entries\n",
    "NUM_LATENT_D = 64                 # Dimension of each codebook entries\n",
    "BETA = 1.0                        # Weight for the commitment loss\n",
    "\n",
    "INPUT_SHAPE = x_train.shape[1:]\n",
    "SIZE = None                       # Spatial size of latent embedding\n",
    "                                  # will be set dynamically in `build_vqvae\n",
    "\n",
    "VQVAE_BATCH_SIZE = 128            # Batch size for training the VQVAE\n",
    "VQVAE_NUM_EPOCHS = 20             # Number of epochs\n",
    "VQVAE_LEARNING_RATE = 3e-4        # Learning rate\n",
    "VQVAE_LAYERS = [16, 32]           # Number of filters for each layer in the encoder\n",
    "\n",
    "PIXELCNN_BATCH_SIZE = 128         # Batch size for training the PixelCNN prior\n",
    "PIXELCNN_NUM_EPOCHS = 10          # Number of epochs\n",
    "PIXELCNN_LEARNING_RATE = 3e-4     # Learning rate\n",
    "PIXELCNN_NUM_BLOCKS = 12          # Number of Gated PixelCNN blocks in the architecture\n",
    "PIXELCNN_NUM_FEATURE_MAPS = 32    # Width of each PixelCNN block\n",
    "\n",
    "IMG_CLASSIFIER='MNIST'            # Trained classifier used to test the accuracy of the reconstructed images (MNIST/FashionMNIST)\n",
    "CATEGORY='0-4'                    # Category of numbers to train the model on ('0-9', '0-4', '5-9')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0c5b8",
   "metadata": {},
   "source": [
    "VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac61773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(layers.Layer):\n",
    "    def __init__(self,num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.beta = (\n",
    "            beta  # This parameter is best kept between [0.25, 2] as per the paper.\n",
    "        )\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping `embedding_dim` intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "\n",
    "        # Quantization.\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
    "        # about adding losses to different layers here:\n",
    "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
    "        # the original paper to get a handle on the formulation of the loss function.\n",
    "        commitment_loss = self.beta * tf.reduce_mean(\n",
    "            (tf.stop_gradient(quantized) - x) ** 2\n",
    "        )\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d75be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(latent_dim=16):\n",
    "    encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
    "        encoder_inputs\n",
    "    )\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    encoder_outputs = layers.Conv2D(latent_dim, 1, padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_decoder(latent_dim=16):\n",
    "    latent_inputs = keras.Input(shape=get_encoder().output.shape[1:])\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
    "        latent_inputs\n",
    "    )\n",
    "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, padding=\"same\")(x)\n",
    "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    \n",
    "\n",
    "#Standalone VQVAE model\n",
    "def get_vqvae(latent_dim=16, num_embeddings=64):\n",
    "    latent_dim=16\n",
    "    num_embeddings=64\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
    "    encoder = get_encoder(latent_dim)\n",
    "    decoder = get_decoder(latent_dim)\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    encoder_outputs = encoder(inputs)\n",
    "    quantized_latents = vq_layer(encoder_outputs)\n",
    "    reconstructions = decoder(quantized_latents)\n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
    "\n",
    "\n",
    "get_vqvae().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784635b1",
   "metadata": {},
   "source": [
    "Wrapping up the training loop inside VQVAETrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAETrainer(keras.models.Model):\n",
    "    def __init__(self, train_variance, latent_dim=16, num_embeddings=128, **kwargs):\n",
    "        super(VQVAETrainer, self).__init__(**kwargs)\n",
    "        self.train_variance = train_variance\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.vqvae = get_vqvae(self.latent_dim, self.num_embeddings)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.vq_loss_tracker,\n",
    "\n",
    "        ]\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Outputs from the VQ-VAE.\n",
    "            reconstructions = self.vqvae(x)\n",
    "            # Calculate the losses.\n",
    "            reconstruction_loss = (\n",
    "                tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
    "            )\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)\n",
    "        \n",
    "        \n",
    "        # Backpropagation.\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Loss tracking.\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "    # Log results.\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A filter to determain the catergory of numbers to train/test the model on\n",
    "def data_cetegory(category ,x_data, y_data):\n",
    "    if (category == '0-9'):\n",
    "        x= x_data\n",
    "        y= y_data      \n",
    "    if (category == '0-4'):\n",
    "        data_filter = np.where((y_data == 0 ) | (y_data == 1 ) | (y_data == 2 ) | (y_data == 3 ) | (y_data == 4 ) )\n",
    "        x, y = x_data[data_filter], y_data[data_filter]\n",
    "    if (category == '5-9'):\n",
    "        data_filter = np.where((y_data == 5 ) | (y_data == 6 ) | (y_data == 7 ) | (y_data == 8 ) | (y_data == 9 ) )\n",
    "        x, y = x_train[data_filter], y_data[data_filter]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75561f21",
   "metadata": {},
   "source": [
    "Train 10 VQ-VAEs with fixed but different 10 seeds (To make the plots comparable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293cf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists for storing data out of the VQ-VAE\n",
    "\n",
    "img_classifications=[]\n",
    "val_loss_all=[]\n",
    "val_loss_all_FashionMNIST=[]\n",
    "val_loss_all_outDist=[]\n",
    "\n",
    "#Initiating 10 different VQ-VAE models with fixed random seeds (The same seed used for the VQVAE with and without quantization)\n",
    "for x in range (10):    \n",
    "\n",
    "    # Seed value\n",
    "    # Using a fixed seed value at each loop\n",
    "    seed_value= x\n",
    "\n",
    "    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "\n",
    "    #Load the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test)  = keras.datasets.mnist.load_data()\n",
    "    \n",
    "    #filter training data according to selected category\n",
    "    x_train, y_train = data_cetegory(CATEGORY ,x_train, y_train)\n",
    "    #filter testing data according to selected category\n",
    "    x_test, y_test = data_cetegory(CATEGORY ,x_test, y_test)\n",
    "\n",
    "    #Preprocessing the data\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    x_train_scaled = (x_train / 255.0) - 0.5\n",
    "    x_test_scaled = (x_test / 255.0) - 0.5\n",
    "    \n",
    "    data_variance = np.var(x_train / 255.0)\n",
    "    test_variance= np. var(x_test / 255.0)\n",
    "    \n",
    "    #Compile and train the model\n",
    "    vqvae_trainer = VQVAETrainer(data_variance, latent_dim=16, num_embeddings=20)\n",
    "    vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
    "    history= vqvae_trainer.fit(x_train_scaled, epochs=20, batch_size=128)    \n",
    "    trained_vqvae_model = vqvae_trainer.vqvae\n",
    "    \n",
    "    #Saving of weights of the model\n",
    "    #trained_vqvae_model.save_weights('trainedVQVAE'+str(x))\n",
    "    \n",
    "    #Loading the saved model weights\n",
    "    trained_vqvae_model.load_weights('trainedVQVAE'+str(x))\n",
    "\n",
    "    #Load pretrained MINST/FashionMNIST classifiers\n",
    "    if IMG_CLASSIFIER=='MNIST'\n",
    "        #pretrained classifier on mnist with accuracy 98%\n",
    "        classifier=load_model('classifier.h5')\n",
    "        else\n",
    "        #pretrained classifier on fashion mnist with accuracy 91%\n",
    "        classifier=load_model('classifierFashionMNIST.h5')\n",
    "    \n",
    "    #Load the fixed 10 patterns of noise images\n",
    "    noise_range=np.load('10noise_ranges'+str(x)+'.npy')\n",
    "\n",
    "    #add noise gradually from 0% to 100% to the test images\n",
    "    for i in range (100):   \n",
    "        test_images = x_test_scaled + noise_range[i]\n",
    "        #predict the output of the VQ-VAE using the moisy image\n",
    "        reconstructions_test = trained_vqvae_model.predict(test_images)\n",
    "        \n",
    "        #an option to precidt the classification of the VQ-VAE's output\n",
    "        #img_classification= classifier.predict(reconstructions_test)\n",
    "        \n",
    "        #an option to collect the classifications\n",
    "        #img_classifications+=[np.argmax(img_classification, axis=1)]\n",
    "        \n",
    "        #an option calculate the classification accuracy of the noisy image against its ground truth test image\n",
    "        #val_loss_noisy=classifier.evaluate(test_images,y_test, verbose=0)\n",
    "        \n",
    "        #calculate the classification accuracy of the output image against its ground truth test image\n",
    "        val_loss_noisy=classifier.evaluate(reconstructions_test,y_test, verbose=0)\n",
    "        \n",
    "        #collect the classifications accuracies\n",
    "        val_loss_all+=[val_loss_noisy[1]]\n",
    "        \n",
    "        #(optional) plot the reconstructions at noise levels 0%, 20%, 40%, 60%, 80% during testing\n",
    "        if (i%20==0):\n",
    "            fig, axs= plt.subplots(5, 1, figsize=(4, 20))\n",
    "            plt.subplot(5, 1, ((i/20)+1)\n",
    "            plt.imshow(reconstructions_test3[0].squeeze() + 0.5, cmap='gray')\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplots_adjust(wspace=0, hspace=0.03)\n",
    "            plt.savefig('59digitVQVAE-09digitTest_fixedSeed_noisePattern'+str(x)+'.png',bbox_inches='tight', dpi=300, linewidth=20, edgecolor='#DFB920')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#collect the classification accuracy of the VQ-VAEs output images agains the test images\n",
    "np.save('VQVAE_accuracies.npy', np.array(val_loss_all))\n",
    "\n",
    "#an option to collect the classification accuracy of the noisy images agains the test images\n",
    "#np.save('noisyImg_accuracies', np.array(val_loss_all))\n",
    "\n",
    "#an option to collect the discrete classifications of the VQ-VAE's output\n",
    "#np.save('imgClassifications.npy', np.array(img_classifications))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot test images vs. reconstructed images (output images)\n",
    "zipped=zip(test_images, reconstructions_test)\n",
    "for idx, x in enumerate(zipped):\n",
    "    show_subplot(test_images[idx], reconstructions_test[idx], idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55957fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot reconstructed images vs. input images\n",
    "def show_subplot(original, reconstructed, idx):\n",
    "    fig, axs= plt.subplots(1, 2, figsize=(8, 4), gridspec_kw={'width_ratios': [1, 1]})\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original.squeeze() + 0.5, cmap='gray')\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    valuesAll=original\n",
    "    colorsAll=valuesAll.flatten()\n",
    "    normAll = Normalize()\n",
    "    normAll.autoscale(colorsAll)\n",
    "    zm = cm.ScalarMappable(cmap='gray')\n",
    "    zm.set_array([])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(reconstructed.squeeze() + 0.5, cmap='gray')\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.savefig('OutOfSample_FashionMNIST_ReconstructedImg'+str(idx)+'.png',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08724ab1",
   "metadata": {},
   "source": [
    "Figure 2 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.load('VQVAE_MNIST_accuracies.npy')\n",
    "b=np.load('autoencoder_MNIST_accuracies.npy')\n",
    "c=np.load('noisytest_MNIST_accuracies.npy')\n",
    "\n",
    "#aevrage the results of 10 runs\n",
    "a_mean=np.mean(np.reshape(a, (-1,100)), axis=0)\n",
    "b_mean=np.mean(np.reshape(b, (-1,100)), axis=0)\n",
    "c_mean=np.mean(np.reshape(c, (-1,100)), axis=0)\n",
    "\n",
    "#calculate the standard deviation of the data to plot the variance\n",
    "a_std=np.std(np.reshape(a,(-1,100)), axis=0, ddof=1)\n",
    "b_std=np.std(np.reshape(b,(-1,100)), axis=0, ddof=1)\n",
    "c_std=np.std(np.reshape(c, (-1,100)), axis=0, ddof=1)\n",
    "\n",
    "x=np.arange(0, 100, 1)\n",
    "\n",
    "fig = plt.figure( dpi=300)\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.errorbar(x,a_mean,  label= 'VQ-VAE output (with quantization)',color='#3F81C0')\n",
    "plt.fill_between(x, a_mean-a_std, a_mean+a_std, alpha=0.3, facecolor='#3F81C0')\n",
    "plt.errorbar(x,b_mean,  label= 'VQ-VAE output (without quantization)',color='#7AA35C')\n",
    "plt.fill_between(x, b_mean-b_std, b_mean+b_std, alpha=0.3, facecolor='#7AA35C')\n",
    "plt.errorbar(x,c_mean,  label= 'MNIST 0-9 input images', linestyle='--',color='#3F81C0')\n",
    "plt.fill_between(x, c_mean-c_std, c_mean+c_std, alpha=0.08, facecolor='#3F81C0')\n",
    "\n",
    "plt.hlines(0.1,0,100, linestyle='dotted', alpha=0.7, color='#3F81C0', label='MNIST chance level')\n",
    "\n",
    "plt.ylim(0,1)\n",
    "x = np.array([0,20,40,60,80,100])\n",
    "y = np.array([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "ticks = ['0%', '20%', '40%', '60%', '80%', '100%']\n",
    "\n",
    "plt.xticks(x, ticks, fontsize=14)\n",
    "plt.yticks(y, ticks, fontsize=14)\n",
    "plt.ylabel('Classification accuracy', fontsize=16)\n",
    "plt.xlabel('Noise level', fontsize=16)\n",
    "\n",
    "#order legend according to the plotted lines\n",
    "matplotlib.colors.to_rgb\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "order = [1,2,3,0]\n",
    "plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=1, fontsize=16)\n",
    "fig.savefig('fig2.tiff',bbox_inches='tight', dpi=300)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a61af",
   "metadata": {},
   "source": [
    "Figure 3 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #04 fashion model - 04 test digits\n",
    "    a=np.load('halfMNIST_inSample.npy')\n",
    "    b=np.load('halfMNIST_outOfSample.npy')\n",
    "    c=np.load('halfMNIST_outOfDist.npy')\n",
    "    f=np.load('halfMNIST_noisyTest.npy')\n",
    "\n",
    "    #04 fashion model - 59 test digits\n",
    "    g=np.load('halfMNIST59_inSample.npy')    \n",
    "    h=np.load('halfMNIST04_outOfSample.npy')    \n",
    "    k=np.load('halfMNISTfashion04_outOfDist.npy')    \n",
    "    l=np.load('halfMNIST59_noisyTest.npy')\n",
    "    \n",
    "    a_concatenated=np.concatenate((a,g), axis=None)\n",
    "    b_concatenated=np.concatenate((b,h), axis=None)\n",
    "    c_concatenated=np.concatenate((c,k), axis=None)\n",
    "    f_concatenated=np.concatenate((f,l), axis=None)\n",
    "    \n",
    "    #aevrage the results of 10 runs\n",
    "    a_mean=np.mean(np.reshape(a_concatenated, (-1,100)), axis=0)\n",
    "    b_mean=np.mean(np.reshape(b_concatenated, (-1,100)), axis=0)\n",
    "    c_mean=np.mean(np.reshape(c_concatenated, (-1,100)), axis=0)\n",
    "    f_mean=np.mean(np.reshape(f_concatenated, (-1,100)), axis=0)\n",
    "    \n",
    "    #calculate the standard deviation of the data to plot the variance\n",
    "    a_std=np.std(np.reshape(a_concatenated,(-1,100)), axis=0, ddof=1)\n",
    "    b_std=np.std(np.reshape(b_concatenated,(-1,100)), axis=0, ddof=1)\n",
    "    c_std=np.std(np.reshape(c_concatenated, (-1,100)), axis=0, ddof=1)\n",
    "    f_std=np.std(np.reshape(f_concatenated, (-1,100)), axis=0, ddof=1)\n",
    "    \n",
    "    x=np.arange(0, 100, 1)\n",
    "    fig = plt.figure( dpi=300)\n",
    "    plt.rcParams[\"font.family\"] = \"Arial\"    \n",
    "    plt.errorbar(x,f_mean,  label= 'Digits 0-4  and 5-9 input images', linestyle='--',color='#3F81C0')\n",
    "    plt.fill_between(x, f_mean-f_std, f_mean+f_std, alpha=0.08, facecolor='#3F81C0')\n",
    "    \n",
    "    plt.hlines(0.1,0,100, linestyle='dotted', alpha=0.7, color='#3F81C0', label='Digits chance level')\n",
    "    plt.hlines(0.01, 0,100, linestyle='None', alpha=0.7,color='white', label=' ')\n",
    "    \n",
    "    plt.errorbar(x,a_mean,  label= 'In-sample digit VQ-VAE',color='#3F81C0')\n",
    "    plt.fill_between(x, a_mean-a_std, a_mean+a_std, alpha=0.25, facecolor='#3F81C0')\n",
    "    plt.errorbar(x,b_mean,  label= 'Out-of-sample digit VQ-VAE',color='#7AA35C')\n",
    "    plt.fill_between(x, b_mean-b_std, b_mean+b_std, alpha=0.25, facecolor='#7AA35C')\n",
    "    plt.errorbar(x,c_mean,  label= 'Out-of-distribution fashion VQ-VAE',zorder=3,color='#DFB920')\n",
    "    plt.fill_between(x, c_mean-c_std, c_mean+c_std, alpha=0.25, facecolor='#DFB920')\n",
    "\n",
    "    plt.ylim(0,1)\n",
    "    x = np.array([0, 20, 40, 60, 80, 100])\n",
    "    y = np.array([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    ticks = ['0%', '20%', '40%', '60%', '80%', '100%']\n",
    "    plt.xticks(x, ticks, fontsize=14)\n",
    "    plt.yticks(y, ticks, fontsize=14)\n",
    "    plt.ylabel('Classification accuracy', fontsize=16)\n",
    "    plt.xlabel('Noise level', fontsize=16)\n",
    "\n",
    "    #order legend according to the plotted lines\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [2,0,1,3,4,5]\n",
    "    plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)#, fontsize=14)\n",
    "    matplotlib.colors.to_rgb\n",
    "    fig.savefig('fig3.tiff',bbox_inches='tight', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8e90d",
   "metadata": {},
   "source": [
    "Figure 4 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dff08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #04 digit model - 04 fashion test\n",
    "    a=np.load('halfFashionMNIST_inSample.npy')\n",
    "    b=np.load('halfFashionMNIST_outOfSample.npy')\n",
    "    c=np.load('halfFashionMNIST_outOfDist.npy')\n",
    "    e=np.load('halfFashionMNIST04_noisyTest.npy')\n",
    "\n",
    "    #04 digit model - 59 fashion model\n",
    "    h=np.load('halfFashionMNIST59_inSample.npy')\n",
    "    g=np.load('halfFashionMNIST04_outOfSample.npy')\n",
    "    k=np.load('halfMNIST04_outOfDist.npy')\n",
    "    l=np.load('halfFashionMNIST59_noisyTest.npy')\n",
    "\n",
    "    a_concatenated=np.concatenate((a,g), axis=None)\n",
    "    b_concatenated=np.concatenate((b,h), axis=None)\n",
    "    c_concatenated=np.concatenate((c,k), axis=None)\n",
    "    e_concatenated=np.concatenate((e,l), axis=None)\n",
    "    \n",
    "    a_mean=np.mean(np.reshape(a_concatenated, (-1,100)), axis=0)\n",
    "    b_mean=np.mean(np.reshape(b_concatenated, (-1,100)), axis=0)\n",
    "    c_mean=np.mean(np.reshape(c_concatenated, (-1,100)), axis=0)\n",
    "    e_mean=np.mean(np.reshape(e_concatenated, (-1,100)), axis=0)\n",
    "    \n",
    "    a_std=np.std(np.reshape(a_concatenated,(-1,100)), axis=0, ddof=1)\n",
    "    b_std=np.std(np.reshape(b_concatenated,(-1,100)), axis=0, ddof=1)\n",
    "    c_std=np.std(np.reshape(c_concatenated, (-1,100)), axis=0, ddof=1)\n",
    "    e_std=np.std(np.reshape(e_concatenated, (-1,100)), axis=0, ddof=1)\n",
    "\n",
    "    x=np.arange(0, 100, 1)\n",
    "    fig = plt.figure( dpi=300)\n",
    " \n",
    "    plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "    \n",
    "    plt.errorbar(x,e_mean,  label= 'Fashion 0-4 and 5-9 input images', linestyle='--',color='#3F81C0')\n",
    "    plt.fill_between(x, e_mean-e_std, e_mean+e_std, alpha=0.08, facecolor='#3F81C0')\n",
    "\n",
    "    plt.hlines(0.1,0,100, linestyle='dotted', alpha=0.7, color='#3F81C0', label='Fashion chance level')\n",
    "    plt.hlines(0.01, 0,100, linestyle='None', alpha=0.7,color='white', label=' ')\n",
    "    \n",
    "    plt.errorbar(x,a_mean,  label= 'In-sample fashion VQ-VAE',color='#3F81C0')\n",
    "    plt.fill_between(x, a_mean-a_std, a_mean+a_std, alpha=0.25, facecolor='#3F81C0')\n",
    "    plt.errorbar(x,b_mean,  label= 'Out-of-sample fashion VQ-VAE',color='#7AA35C')\n",
    "    plt.fill_between(x, b_mean-b_std, b_mean+b_std, alpha=0.25, facecolor='#7AA35C')\n",
    "    plt.errorbar(x,c_mean,  label= 'Out-of-distribution digit VQ-VAE',zorder=3,color='#DFB920')\n",
    "    plt.fill_between(x, c_mean-c_std, c_mean+c_std, alpha=0.25, facecolor='#DFB920')\n",
    "\n",
    "    plt.ylim(0,1)\n",
    "    x = np.array([0, 20, 40, 60, 80, 100])\n",
    "    y = np.array([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    ticks = ['0%', '20%', '40%', '60%', '80%', '100%']\n",
    "    plt.xticks(x, ticks)\n",
    "    plt.yticks(y, ticks)\n",
    "    plt.ylabel('Classification accuracy')\n",
    "    plt.xlabel('Noise level')\n",
    " \n",
    "    #order legend according to the plotted lines\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [2,0,1,3,4,5]\n",
    "    plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "    matplotlib.colors.to_rgb\n",
    "    fig.savefig('fig4.tiff',bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
